<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><entry><title>.NET, Go, Kamelets, and more: Top articles from November 2022</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/11/24/net-go-kamelets-and-more-top-articles-november-2022" /><author><name>Heiker Medina</name></author><id>8a3c16cc-51fd-42d2-bf3d-741065b76de7</id><updated>2022-11-24T07:00:00Z</updated><published>2022-11-24T07:00:00Z</published><summary type="html">&lt;p&gt;Whether you'll soon be signing off for a fall November break or working through the end of the month, take a moment to check out Red Hat Developer's latest top-performing articles. We've highlighted the tutorial guides and announcements that our developer community has engaged with the most.&lt;/p&gt; &lt;h2&gt;Announcements&lt;/h2&gt; &lt;ul&gt;&lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://developers.redhat.com/articles/2022/11/08/net-7-now-available-rhel-and-openshift"&gt;.NET 7 now available for RHEL and OpenShift&lt;/a&gt;:&lt;/strong&gt; This short overview from John Clingan discusses what you need to know about &lt;a href="https://developers.redhat.com/topics/dotnet"&gt;.NET&lt;/a&gt; 7. The .NET 7 release is now available, targeting &lt;a href="https://developers.redhat.com/products/rhel/overview"&gt;Red Hat Enterprise Linux&lt;/a&gt; (RHEL) 8.7, RHEL 9.1, and &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://developers.redhat.com/articles/2022/11/04/intros-deep-dives-and-announcements-our-best-october-2022"&gt;Intros, deep dives, and announcements: Our best of October 2022&lt;/a&gt;:&lt;/strong&gt; Here are some highlights from Red Hat Developer for October 2022, organized by product announcements, topic roundups, learning guides, and advanced deep dives&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Tutorials&lt;/h2&gt; &lt;ul&gt;&lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://developers.redhat.com/articles/2022/11/14/3-ways-embed-commit-hash-go-programs"&gt;3 ways to embed a commit hash in Go programs&lt;/a&gt;:&lt;/strong&gt; When you need to view the source code of an older version of your software, it is handy to have the history feature turned on. Panagiotis Georgiadis's guide explains how to see what your &lt;a href="https://developers.redhat.com/topics/go"&gt;Go&lt;/a&gt; program looked like at various points in time, so you can debug issues that may have been introduced at specific points.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://developers.redhat.com/articles/2022/11/09/how-static-application-security-testing-improves-software-security"&gt;How static application security testing improves software security&lt;/a&gt;:&lt;/strong&gt; Join Florencio Cano Gabarda as he explains why static application security testing (SAST) is an effective tool for improving the security of your application. Developers can use SAST to identify potential security problems in the source code for an application, its bytecode, or its binary code. Many SAST tools are mature and widely used by software developers.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://developers.redhat.com/articles/2022/11/08/introduction-debug-events-learn-how-use-breakpoints"&gt;An introduction to debug events: Learn how to use breakpoints&lt;/a&gt;:&lt;/strong&gt; This article kicks off a series about GDB's debugging capabilities. Keith Seitz will be covering the commands, convenience variables, and functions that will aid you in stopping GDB at the right place at the right time.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://developers.redhat.com/articles/2022/11/01/how-kamelets-simplify-camel-integrations-kubernetes"&gt;How Kamelets simplify Camel integrations on Kubernetes&lt;/a&gt;:&lt;/strong&gt; Modern applications are often made of a collection of several smaller applications, or subsystems. This article explains how Apache Camel and &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; make it easy to integrate such services through Kamelets.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://developers.redhat.com/articles/2022/11/07/build-reactive-apps-kubernetes-using-camel-k"&gt;Build reactive apps on Kubernetes using Camel K&lt;/a&gt;:&lt;/strong&gt; In this article, Sumit Mukherjee will explain how &lt;a href="https://developers.redhat.com/topics/camel-k"&gt;Apache Camel K&lt;/a&gt; can make it easier to develop reactive applications on Kubernetes by integrating data sources, &lt;a href="https://developers.redhat.com/topics/kafka-kubernetes"&gt;Apache Kafka&lt;/a&gt; brokers, and Knative for event management.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://developers.redhat.com/articles/2022/11/03/nodejs-reference-architecture-part-10-accessibility"&gt;Node.js Reference Architecture, Part 10: Accessibility&lt;/a&gt;:&lt;/strong&gt; Michael Dawson guides you through the importance of integrating accessibility within your &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js&lt;/a&gt; applications. Making your applications accessible to disabled users is good business and often required by law. As a Node.js developer, you need to understand the issues around accessibility so that you can build truly accessible components into the applications you build.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://developers.redhat.com/articles/2022/11/10/set-openshift-cluster-deploy-application-odo-cli"&gt;Set up an OpenShift cluster to deploy an application in odo CLI&lt;/a&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;a href="https://developers.redhat.com/articles/2022/11/10/implement-restapi-application-mongodb-using-sbo"&gt;Implement a Rest API application with MongoDB using SBO&lt;/a&gt;:&lt;/strong&gt; In this two-part series, Francesco Ilario shows you how to use &lt;code&gt;odo&lt;/code&gt; to create an application and a database service, bind the application to the database using the Service Binding Operator, and get access to the application's REST API. You'll deploy a REST API application and bind it to a MongoDB using &lt;code&gt;odo&lt;/code&gt; and the Service Binding Operator. The series shows you how easy it is to create an instance of a database and a binding (connection) between the application and that database.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;November 2022 on Red Hat Developer&lt;/h2&gt; &lt;p&gt;Here's the full lineup of articles published on Red Hat Developer this month:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/11/21/why-gpus-are-essential-computing"&gt;Why GPUs are essential for AI and high-performance computing&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/11/18/modernize-scale-new-migration-toolkit-applications"&gt;Modernize at scale with the new migration toolkit for applications&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/11/17/new-features-openmp-51-and-openmp-52"&gt;New features in OpenMP 5.1 and OpenMP 5.2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/11/17/benchmarking-improved-conntrack-performance-ovs-300"&gt;Benchmarking improved conntrack performance in OvS 3.0.0&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/11/16/whats-new-red-hat-enterprise-linux-91"&gt;What's new in Red Hat Enterprise Linux 9.1&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/11/16/openshift-essential-containerized-applications"&gt;Why OpenShift is essential for containerized applications&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/11/15/knative-broker-enhances-kafka-openshift-serverless"&gt;How Knative broker GA enhances Kafka on OpenShift Serverless&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/11/15/how-categorize-c-programs-behavior"&gt;How to categorize C programs by behavior&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/11/14/3-ways-embed-commit-hash-go-programs"&gt;3 ways to embed a commit hash in Go programs&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/11/10/implement-restapi-application-mongodb-using-sbo"&gt;Implement a Rest API application with MongoDB using SBO&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/11/10/set-openshift-cluster-deploy-application-odo-cli"&gt;Set up an OpenShift cluster to deploy an application in odo CLI&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/11/09/how-static-application-security-testing-improves-software-security"&gt;How static application security testing improves software security&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/11/08/net-7-now-available-rhel-and-openshift"&gt;.NET 7 now available for RHEL and OpenShift&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/11/08/visual-guide-deploying-jboss-eap-aws"&gt;A visual guide to deploying JBoss EAP on AWS&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/11/08/introduction-debug-events-learn-how-use-breakpoints"&gt;An introduction to debug events: Learn how to use breakpoints&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/11/07/build-reactive-apps-kubernetes-using-camel-k"&gt;Build reactive apps on Kubernetes using Camel K&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/11/03/nodejs-reference-architecture-part-10-accessibility"&gt;Node.js Reference Architecture, Part 10: Accessibility&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/11/03/how-next-10-project-supports-future-nodejs"&gt;How the Next-10 project supports the future of Node.js&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/11/01/how-kamelets-simplify-camel-integrations-kubernetes"&gt;How Kamelets simplify Camel integrations on Kubernetes&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/10/31/best-practices-application-shutdown-openssl"&gt;Best practices for application shutdown with OpenSSL&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/11/24/net-go-kamelets-and-more-top-articles-november-2022" title=".NET, Go, Kamelets, and more: Top articles from November 2022"&gt;.NET, Go, Kamelets, and more: Top articles from November 2022&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Heiker Medina</dc:creator><dc:date>2022-11-24T07:00:00Z</dc:date></entry><entry><title type="html">Reactive CRUD Performance: A Case Study</title><link rel="alternate" href="https://quarkus.io/blog/reactive-crud-performance-case-study/" /><author><name>John O'Hara</name></author><id>https://quarkus.io/blog/reactive-crud-performance-case-study/</id><updated>2022-11-23T23:00:00Z</updated><content type="html">We were approached for comment about the relative performance of Quarkus for a reactive CRUD workload. This is a good case study into performance test design and some of the considerations required and hurdles that need to be overcome. What methodology can we derive for ensuring that the test we...</content><dc:creator>John O'Hara</dc:creator></entry><entry><title type="html">Kogito Serverless Workflow event formats</title><link rel="alternate" href="https://blog.kie.org/2022/11/kogito-serverless-workflow-event-formats.html" /><author><name>Francisco Javier Tirado Sarti</name></author><id>https://blog.kie.org/2022/11/kogito-serverless-workflow-event-formats.html</id><updated>2022-11-23T18:56:28Z</updated><content type="html">The relies on for event publishing and consumption. CloudEvents are designed in a way that might work with any event format. That goal is achieved by declaring the data property, the one containing the event information, as an array of bytes.  expects that incoming and outgoing events represent a CloudEvent and that its data property content is convertible to a JSON object. The process of converting the CloudEvent to an object that can be understood by an external event broker is called marshaling. The inverse procedure, the one that converts the external event broker object into a CloudEvent is called unmarshaling. They are usually, but not always, coupled and I will refer to both of them as (un)marshal procedure for brevity.  When Kogito is running on the Quarkus platform, integration with external event brokers is performed through, as described in this .  The main abstraction provided by a Smallrye connector is the channel. By default, Kogito assumes that all channels within a Workflow application use the same logic for marshaling and unmarshaling. However, in complex applications, the event format used by a channel might be different from the one used by other channels within the same application, therefore Kogito provides means to specify which (un)marshal procedure should be used for each channel.  This post describes (un)marshal procedures provided by Kogito Serverless Workflow out of the box and focuses on how to set up a workflow application to use them, either globally or channel specific. It also discusses how to add new (un)marshal procedures to Kogito Serverless Workflows programmatically, if the included ones are not suitable. The only thing you will need is basic Java knowledge and some familiarity with J2EE CDI functionality, specifically the annotation.  APPLICATION (UN)MARSHALER As mentioned previously, if nothing is configured, Kogito Serverless workflow assumes all channels within the same application utilize the same (un)marshall procedure, based on library. This means that Jackson parser should be able to convert the Smallrye message payload into a JSON object without errors and vice versa.  Let’s assume you want your application to use a different event format. We are going to describe how to do that for , taking advantage of the fact that Kogito Serverless provides out of the box an (un)marshaler based on the CloudEvent specification . However, you should be aware that the same procedure can be used to configure  any  other (un)marshaler.  There are the steps you need to follow: 1. Add kogito-marshallers-avro dependency to your pom.xml &lt;dependency&gt;   &lt;groupId&gt;org.kie.kogito&lt;/groupId&gt;   &lt;artifactId&gt;kogito-addons-quarkus-marshallers-avro&lt;/artifactId&gt; &lt;/dependency&gt; 2. Define, under /src/main/java, a bean factory class that creates the desired bean definitions for and interfaces. In the class below, we are using the implementations provided by Kogito marshaller addon included as dependency in the previous step.  @ApplicationScoped public class ApplicationMarshallerProducer {     private AvroIO avroIO;     @PostConstruct     void init() throws IOException {             avroIO = new AvroIO();     }     @Produces     public CloudEventUnmarshallerFactory&lt;byte[]&gt; getAvroCloudEventUnmarshallerFactory() {             return new AvroCloudEventUnmarshallerFactory(avroIO);     }     @Produces     public CloudEventMarshaller&lt;byte[]&gt; getAvroCloudEventMarshaller() {               return new AvroCloudEventMarshaller(avroIO);     } } The previous setup assumes that all messages have as payload an array of bytes. When using Kafka, this is achieved by using the proper serializer and deserializer. In order to do that, you should  set these properties for every channel: mp.messaging.outgoing.&lt;channelName&gt;.value.deserializer=org.apache.kafka.common.serialization.ByteArrayDeserializer mp.messaging.outgoing.&lt;channelName&gt;.value.serializer=org.apache.kafka.common.serialization.ByteArraySerializer You might be wondering why you need to add a Java class to set up the global marshaller. This is intentional to allow flexibility. For example, you might want your application consumes Avro event format and republish them in Json event format. To do that, you just need to set up the marshaller to be the Jackson one and unmarshaller to be the Avro one, using a Java class as below.  @ApplicationScoped public class ApplicationMarshallerProducer {         @Inject     ObjectMapper objectMapper;     private AvroIO avroIO;        @PostConstruct     void init() throws IOException {             avroIO = new AvroIO();     }     @Produces     public CloudEventUnmarshallerFactory&lt;byte[]&gt; getAvroCloudEventUnmarshallerFactory() {             return new AvroCloudEventUnmarshallerFactory(avroIO);     }     @Produces     public CloudEventMarshaller&lt;byte[]&gt; getJacksonCloudEventMarshaller()      {               return new ByteArrayEventMarshaller(objectMapper);     } } PER CHANNEL (UN)MARSHALER You have learned how to set up the application level (un)marshaler procedure, but what happens if your application defines several incoming channels, one of them is expecting events to arrive in avro format, and the other one expects json format? The answer is pretty easy, since the default (un)marshaller procedure is based on Jackson, you just need to configure the channel that consumes Avro events to use the Avro unmarshaller.  In order to do that, you need to perform the following steps: 1. Add kogito-marshallers-avro dependency to your pom.xml &lt;dependency&gt;   &lt;groupId&gt;org.kie.kogito&lt;/groupId&gt;   &lt;artifactId&gt;kogito-addons-quarkus-marshallers-avro&lt;/artifactId&gt; &lt;/dependency&gt; 2. Define a named bean for the CloudEventUnmarshallerFactory interface, annotated with ChannelFormat annotation. In the class below,  the bean name is “avro” and the implementation is the one provided by the Kogito marshaller addon.  @ApplicationScoped public class AvroMarshallerProducer {     private AvroIO avroIO;     @PostConstruct     void init() throws IOException {         avroIO = new AvroIO();     }     @Produces     @Named(“avro”)     @ChannelFormat     public CloudEventUnmarshallerFactory&lt;byte[]&gt; getAvroCloudEventUnmarshallerFactory() {         return new AvroCloudEventUnmarshallerFactory(avroIO);     } } 3. Add a property that establishes the mapping between the channel and the bean name. The property formats are: *  kogito.addon.messaging.unmarshaller.&lt;channelName&gt;=&lt;beanName&gt; for incoming  channels. *  kogito.addon.messaging.marshaller.&lt;channelName&gt;=&lt;beanName&gt; for outgoing channels. Notice that you can map several channels to the same bean. For example, if the channel name is applicants, since your channel is incoming, you need to add this line to application.properties kogito.addon.messaging.unmarshaller.applicants=avro If your application had two incoming channels using avro, you would need to add: kogito.addon.messaging.unmarshaller.newApplicants=avro kogito.addon.messaging.unmarshaller.legacyAppicant=avro You can  find a serverless workflow application using Avro and Json for incoming channels in the Kogito examples .  ADDING CUSTOM MARSHALLERS You already know how to set up application and channel level (un)marshaller procedures using predefined Kogito ones: Jackson and Avro, but what happens if your channels use a different format? In that case, you need to provide your own implementation of the (un)marshaller procedure.  You probably are already aware that setting up a custom (un)marshaller procedure is equivalent to using a predefined one. The difference is that rather than including an existing Kogito addon as dependency in your pom and utilize the classes defined there to produce the required CDI beans, you need to develop your own classes and use them as CDI beans instead. Therefore this section explains which Kogito interfaces need to be implemented to do so. It is assumed that you are fluent in Java.  There are three interfaces to implement, CloudEventMarshaller, and CloudEventUnmarshallerFactory. You can use the as a reference to follow the explanation in the paragraphs below.  UNMARSHALLER IMPLEMENTATION CloudEventUnmarshallerFactory is responsible for creating CloudEventUnmarshaller instances suitable for the provided class parameter, which in the case of Serverless Workflow is always (remember that Kogito is intended to work also with BPMN, which uses ) Therefore, let’s focus on CloudEventUnmarshaller, which is responsible for converting the message payload into a CloudEvent and its data property into a JsonNode.  public interface CloudEventUnmarshaller&lt;I, O&gt; {     /**     * Create Cloud Event from structure event payload     *     * @return Cloud Event     */     Converter&lt;I, CloudEvent&gt; cloudEvent();     /**     * Create Cloud Event from binary event payload     *     * @return Cloud Event Data     */     Converter&lt;I, CloudEventData&gt; binaryCloudEvent();     /**     * Creates Kogito business object from Cloud Event data     *     * @return Kogito Businnes Object     */     Converter&lt;CloudEventData, O&gt; data(); } Let’s start first with the generic type I, which represents the possible message payloads. Currently, there are three of them: String, byte[] and Object. Note that Avro (un)marshaller procedure assumes that it is a byte[]. Generic type O is the target object type, which, as mentioned, is always JsonNode for Serverless Workflow.  Finally, there is the interface, which is a checked version of . This interface is responsible for converting from source type to target type and throws an if there is any conversion issue. An unmarshaller implementation should provide three converters: 1. cloudEvent method converter is used when the CloudEvent is delivered as   2. binaryCloudEvent method converter is used when the CloudEvent is delivered as .  3. data method converter is used regardless of the mode message to convert CloudEventData into the target object type.  MARSHALLER IMPLEMENTATION  CloudEventMarshaller interface is responsible for transforming the CloudEvent into a message payload. Note that Kogito assumes structure mode for publishing.  public interface CloudEventMarshaller&lt;R&gt; {     /**     * Convert cloud event into the type expected by the external service     *     * @param event Cloud event to be converted     * @return object to be sent to the external service     * @throws IOException if there is a conversion problem. This method must NOT report event formatting issues through a runtime exception, it must use IOException. This way the caller     *         can differentiate between unexpected issues and event formatting ones.     */     R marshall(CloudEvent event) throws IOException;     /**     * Convert Kogito business object into a CloudEventData for marshaling     *     * @param &lt;T&gt; the Kogito business object type     * @return A CloudEventData that will be marshaled.     */     &lt;T&gt; Function&lt;T, CloudEventData&gt; cloudEventDataFactory(); } GenericType R represents the target message payload type. Possible types are byte[], String and Object.  There are two methods to implement:  1. Marshall methods convert the CloudEvent into the target message payload.  2. cloudEventDataFactory is used internally by Kogito when a CloudEvent is built, for publishing  purposes, to fill its data property. It  converts the Kogito business object (JsonNode in the case of a Serverless Workflow), represented by generic type T, into a CloudEventData instance. This method belongs to the marshaller interface because the CloudEventData implementation to be part of the CloudEvent usually depends on the marshaling procedure.  The post appeared first on .</content><dc:creator>Francisco Javier Tirado Sarti</dc:creator></entry><entry><title>How to deploy Next.js applications to Red Hat OpenShift</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/11/23/how-deploy-nextjs-applications-red-hat-openshift" /><author><name>Michael Dawson</name></author><id>eeff6449-d837-4fc1-bc79-4a7a8fdd9434</id><updated>2022-11-23T07:00:00Z</updated><published>2022-11-23T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://nextjs.org/"&gt;Next.js&lt;/a&gt; is a popular &lt;em&gt;framework&lt;/em&gt; for deploying sites based on &lt;a href="https://nodejs.org/en/"&gt;Node.js&lt;/a&gt;. You can read about many popular Node.js frameworks in &lt;a href="https://developers.redhat.com/articles/2021/12/03/introduction-nodejs-reference-architecture-part-6-choosing-web-frameworks"&gt;part 6&lt;/a&gt; of the Red Hat Developer series on the &lt;a href="https://developers.redhat.com/blog/2021/03/08/introduction-to-the-node-js-reference-architecture-part-1-overview"&gt;Node.js reference architecture&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;In this article, you'll learn how to deploy Next.js applications using the &lt;a href="https://catalog.redhat.com/software/containers/ubi8/nodejs-16/615aee9fc739c0a4123a87e1"&gt;ubi8/nodejs-16&lt;/a&gt; and &lt;a href="https://catalog.redhat.com/software/containers/ubi8/nodejs-16-minimal/615aefd53f6014fa45ae1ae2"&gt;ubi8/nodejs-16-minimal&lt;/a&gt; containers available from Red Hat.&lt;/p&gt; &lt;p&gt;If you are a Red Hat customer with support, then you'll want to use the UBI or RHEL containers; this article was inspired in part because one of our customers asked us how to do that with Next.js. Even if you are not a customer yet, ubi8 containers are free to use and a great choice; we've seen their usage continue to grow.&lt;/p&gt; &lt;p&gt;Once you've seen how to build a Next.js application on top of the UBI containers, we'll show you how you can do the build and run on the &lt;a href="https://developers.redhat.com/products/openshift/getting-started"&gt;Red Hat OpenShift&lt;/a&gt; Container Platform, which is a great way to deploy your applications in a hybrid cloud environment.&lt;/p&gt; &lt;h2&gt;Using Next.js with or without Node.js?&lt;/h2&gt; &lt;p&gt;Like a number of other frameworks, Next.js can export your application as &lt;a href="https://nextjs.org/docs/advanced-features/static-html-export"&gt;static HTML&lt;/a&gt;. However, only a subset of features are supported, with serverless functions and server-side rendering being two major features you wouldn't be able to use. If you are deploying as static HTML only, you most likely want to use Nginx to serve that static HTML. The Red Hat Developer article &lt;a href="https://developers.redhat.com/blog/2018/10/23/modern-web-applications-on-openshift-part-2-using-chained-builds#"&gt;Modern web applications on OpenShift: Using chained builds&lt;/a&gt; shows you how you can do that with Nginx and OpenShift.&lt;/p&gt; &lt;p&gt;In this article, though, we'll assume you do want to use features like serverless functions and server-side rendering. That means you need Node.js to run your Next.js application in production.&lt;/p&gt; &lt;h2&gt;Building a container to run your Next.js application&lt;/h2&gt; &lt;p&gt;While the stewards of Next.js would like you to run in their environment, there are reasons you might not want to do so. Some organizations want more control over their deployments and either prefer to host them on their own infrastructure or with a cloud provider of their choice.&lt;/p&gt; &lt;p&gt;The good news is that there is some information on building containers to host a Next.js application in the &lt;a href="https://github.com/vercel/next.js/tree/canary/examples/with-docker"&gt;examples&lt;/a&gt; that Next.js provides. We can use that pattern and adapt it to use the ubi8 Node.js containers using a &lt;em&gt;multi-stage build.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;A multi-stage build is a best practice that uses a larger builder image during the build and a smaller deployment image for the final container. This works because not all components that are needed during the build are required for deployment. For example, a C++ compiler that may be needed to build a native add-on would not ultimately be needed when you deploy the application. The result is that the end container can be much smaller than it would be if the build container were used for the final image. You can read more about this &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture/blob/main/docs/development/building-good-containers.md#use-multi-stage-builds"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;In order to build the Next.js application into a container, we need the applications files, a Dockerfile, and a Next.js configuration file that tells Next.js to build for a stand-alone deployment. Here's what the Next.js configuration file, called &lt;code&gt;next.config.js&lt;/code&gt;, should look like:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;/** @type {import('next').NextConfig} */ module.exports = { output: 'standalone', }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The Dockerfile is as shown below. We will explain each section after the listing.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# adapted from https://github.com/vercel/next.js/tree/canary/examples/with-docker # needs next.config.js to set build to stand-alone with context as follows # /** @type {import('next').NextConfig} */ # module.exports = { # output: 'standalone', # } # Recommended to have .dockerignore file with the following content # Dockerfile # .dockerignore # node_modules # npm-debug.log # README.md # .next # .git # Install dependencies only when needed FROM registry.access.redhat.com/ubi8/nodejs-16 AS deps USER 0 WORKDIR /app # Install dependencies based on the preferred package manager COPY package.json yarn.lock* package-lock.json* pnpm-lock.yaml* ./ RUN \ if [ -f yarn.lock ]; then yarn --frozen-lockfile; \ elif [ -f package-lock.json ]; then npm ci; \ elif [ -f pnpm-lock.yaml ]; then yarn global add pnpm &amp;&amp; pnpm i; \ else echo "Lockfile not found." &amp;&amp; exit 1; \ fi # Rebuild the source code only when needed FROM registry.access.redhat.com/ubi8/nodejs-16 AS builder USER 0 WORKDIR /app COPY --from=deps /app/node_modules ./node_modules COPY . . # Next.js collects completely anonymous telemetry data about general usage. # Learn more here: https://nextjs.org/telemetry # Uncomment the following line in case you want to disable telemetry during the build. ENV NEXT_TELEMETRY_DISABLED 1 # If using yarn uncomment out and comment out npm below # RUN yarn build # If using npm comment out above and use below instead RUN npm run build # Production image, copy all the files and run next FROM registry.access.redhat.com/ubi8/nodejs-16-minimal AS runner USER 0 WORKDIR /app ENV NODE_ENV production # Uncomment the following line in case you want to enable telemetry during runtime. ENV NEXT_TELEMETRY_DISABLED 1 COPY --from=builder /app/public ./public # Automatically leverage output traces to reduce image size # https://nextjs.org/docs/advanced-features/output-file-tracing COPY --from=builder --chown=1001:1001 /app/.next/standalone ./ COPY --from=builder --chown=1001:1001 /app/.next/static ./.next/static USER 1001 EXPOSE 3000 ENV PORT 3000 CMD ["node", "server.js"]&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The Dockerfile builds two intermediate layers (&lt;code&gt;deps&lt;/code&gt; and &lt;code&gt;builder&lt;/code&gt;) and then the final image that we will deploy.&lt;/p&gt; &lt;p&gt;The first section installs the dependencies in the &lt;code&gt;deps&lt;/code&gt; layer. The &lt;code&gt;FROM&lt;/code&gt; line indicates that ubi8/nodejs-16 should be used as the base image for the layer.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# Install dependencies only when needed FROM registry.access.redhat.com/ubi8/nodejs-16 AS deps USER 0 WORKDIR /app # Install dependencies based on the preferred package manager COPY package.json yarn.lock* package-lock.json* pnpm-lock.yaml* ./ RUN \ if [ -f yarn.lock ]; then yarn --frozen-lockfile; \ elif [ -f package-lock.json ]; then npm ci; \ elif [ -f pnpm-lock.yaml ]; then yarn global add pnpm &amp;&amp; pnpm i; \ else echo "Lockfile not found." &amp;&amp; exit 1; \ fi &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The second part copies over the dependencies and then builds the application as a builder layer. Again, we use ubi8/nodejs-16 as the base image. We've also chosen to disable telemetry and to build with &lt;code&gt;npm&lt;/code&gt;.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;​​​​​​​# Rebuild the source code only when needed FROM registry.access.redhat.com/ubi8/nodejs-16 AS builder USER 0 WORKDIR /app COPY --from=deps /app/node_modules ./node_modules COPY . . # Next.js collects completely anonymous telemetry data about general usage. # Learn more here: https://nextjs.org/telemetry # Uncomment the following line in case you want to enable telemetry during the build. ENV NEXT_TELEMETRY_DISABLED 1 # If using yarn uncomment out and comment out npm below # RUN yarn build # If using npm comment out above and use below instead RUN npm run build&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In the final step, we use the ubi8/nodejs-16-minimal image, as we don't need the tools and packages required for the build. Then we copy over the components that were built to create the final minimal image that will run the application.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# Production image, copy all the files and run next FROM registry.access.redhat.com/ubi8/nodejs-16-minimal AS runner USER 0 WORKDIR /app ENV NODE_ENV production # Uncomment the following line in case you want to enable telemetry during runtime. ENV NEXT_TELEMETRY_DISABLED 1 COPY --from=builder /app/public ./public # Automatically leverage output traces to reduce image size # https://nextjs.org/docs/advanced-features/output-file-tracing COPY --from=builder --chown=1001:1001 /app/.next/standalone ./ COPY --from=builder --chown=1001:1001 /app/.next/static ./.next/static USER 1001 EXPOSE 3000 ENV PORT 3000 CMD ["node", "server.js"]&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In each of these steps, we set the user to 0 with &lt;code&gt;USER 0&lt;/code&gt; at the start so that we can copy to the desired locations. This is necessary because the UBI Node.js images have the user set to 1001 by default for usage with &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_software_collections/2/html/using_red_hat_software_collections_container_images/sti"&gt;Source-to-Image&lt;/a&gt;. In the final step, we set the user back to 1001, as we don't want our container to run as root. User 1001 is a user that has been added to the UBI Node.js containers as a user suitable for running the application.&lt;/p&gt; &lt;h2&gt;Trying it out&lt;/h2&gt; &lt;p&gt;You can try building and running a sample application with the Dockerfile by following the steps below. For those running on Fedora or RHEL, you can replace the &lt;code&gt;docker&lt;/code&gt; command with &lt;code&gt;podman&lt;/code&gt;.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;npx create-next-app --example with-docker myapp cp Dockerfile myapp cp next.config.js myapp cd myapp docker build . -t ubi-nextjs&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This should result in an image tagged with &lt;code&gt;ubi-nextjs:latest&lt;/code&gt;. You can then run that image with:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;​​​​​​​docker run -p 3000:3000 ubi-nextjs&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You should see the following output and then be able to connect with your web browser to port 3000:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;​​​​​​​Listening on port 3000 url: http://localhost:3000&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You now have your first Next.js application running on the ubi8/nodejs-16 docker image!&lt;/p&gt; &lt;h2&gt;Deployment to OpenShift&lt;/h2&gt; &lt;p&gt;The next step is to build and deploy the Next.js application in OpenShift. There are many other ways to build and deploy in OpenShift—with &lt;a href="https://developers.redhat.com/blog/2021/01/13/getting-started-with-tekton-and-pipelines"&gt;Tekton&lt;/a&gt;, for instance—but for this example we'll stick to one of the easiest: the trio of an &lt;code&gt;ImageStream&lt;/code&gt;, &lt;code&gt;BuildConfig&lt;/code&gt;, and &lt;code&gt;Deployment&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;We'll also need a place to get the application from, and a Git repository is an easy place so that is what we'll use. You can push your Next.js application along with the Dockerfile and Next.js configuration file to a GitHub repository; I used &lt;a href="https://github.com/nodeshift-blog-examples/ubi8-nextjs"&gt;this one&lt;/a&gt;. The &lt;code&gt;BuildConfig&lt;/code&gt; will pull the application from that repository as part of the build.&lt;/p&gt; &lt;p&gt;An &lt;code&gt;ImageStream&lt;/code&gt; provides us an easy place to store our Docker image once it's built. Once you've logged into OpenShift, you can create one by adding the following content to &lt;code&gt;image-stream.yaml&lt;/code&gt; and then running &lt;code&gt;oc apply -f image-stream.yaml&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;kind: ImageStream apiVersion: image.openshift.io/v1 metadata: name: ubi8-nextjs namespace: default&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This will create a new &lt;code&gt;ImageStream&lt;/code&gt; named &lt;code&gt;ubi8-nextjs&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;The next step is to create a &lt;code&gt;BuildConfig&lt;/code&gt; that will build our application from the Dockerfile we discussed earlier and push it to the &lt;code&gt;ubi8-nextjs&lt;/code&gt; &lt;code&gt;ImageStream&lt;/code&gt;. You can do that by adding the following content to &lt;code&gt;build-config.yaml&lt;/code&gt; and running &lt;code&gt;oc apply -f build-config.yaml&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;apiVersion: build.openshift.io/v1 kind: BuildConfig metadata: name: ubi8-nextjs labels: app: ubi8-nextjs spec: source: type: Git git: uri: https://github.com/nodeshift-blog-examples/ubi8-nextjs contextDir: strategy: type: Docker dockerStrategy: dockerfilePath: Dockerfile # Look for Dockerfile in: gitUri/contextDir/dockerfilePath output: to: kind: ImageStreamTag name: ubi8-nextjs:latest&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This &lt;code&gt;BuildConfig&lt;/code&gt; will clone the contents of a GitHub repository for your application and then build it using the Dockerfile at the root of the application. Once built, the container is pushed to the &lt;code&gt;ubi8-nextjs&lt;/code&gt; &lt;code&gt;ImageStream&lt;/code&gt; with the tag &lt;code&gt;latest&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;​​​​​​​Once you've applied the YAML for the &lt;code&gt;BuildConfig&lt;/code&gt;, you can go to the OpenShift UI, select the &lt;code&gt;ubi8-next&lt;/code&gt; &lt;code&gt;BuildConfig&lt;/code&gt; and then select &lt;strong&gt;Start build&lt;/strong&gt; under the &lt;strong&gt;Actions&lt;/strong&gt; dropdown, as shown in Figure 1. (Of course you can do this from the command line as well.)&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/fig1_22.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/fig1_22.png?itok=SNtT3Q1C" width="600" height="445" alt="Screenshot showing how to start a build from the OpenShift UI." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: Starting a build. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;Once the build completes, the same Docker image that you built locally earlier is now stored in the &lt;code&gt;ImageStream&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;The last step is to deploy the application from the &lt;code&gt;ImageStream&lt;/code&gt;. To do that, we'll use a &lt;code&gt;Deployment&lt;/code&gt;, which we can do either through the UI or some more YAML. The easiest way is through the UI, as it will automatically create additional resources like services that we need to access the application.&lt;/p&gt; &lt;p&gt;Navigate to the &lt;strong&gt;Developer Topology&lt;/strong&gt; view and select the &lt;strong&gt;Add Page&lt;/strong&gt; link. From the &lt;strong&gt;Add&lt;/strong&gt; page, select &lt;strong&gt;Container images&lt;/strong&gt;. From the &lt;strong&gt;Deploy image&lt;/strong&gt; page that pops up (Figure 2), select &lt;strong&gt;Image stream tag from internal registry&lt;/strong&gt;, and then &lt;strong&gt;ubi8-nextjs&lt;/strong&gt; for the &lt;strong&gt;Image Stream&lt;/strong&gt; and &lt;strong&gt;latest&lt;/strong&gt; for the &lt;strong&gt;Tag&lt;/strong&gt;. Scroll down and set the &lt;strong&gt;Target port&lt;/strong&gt; to match the value in our Dockerfile, which is 3000. Accept the rest of the default values and select &lt;strong&gt;Create&lt;/strong&gt;.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/fig2_15.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/fig2_15.png?itok=MT201kNB" width="600" height="590" alt="Screenshot showing how to deploy an image from the OpenShift UI." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: Deploying an image from the OpenShift UI. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;You should then return to the &lt;strong&gt;Topology&lt;/strong&gt; page, where you will see the running application, as in Figure 3.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/fig3_7.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/fig3_7.png?itok=QjEBgjbk" width="600" height="362" alt="Screenshot showing that the running application is visible on the Topology page." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3: The running application is visible on the Topology page. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt;OpenShift automatically creates the service and route needed to access the application. Just click on the application to get more information and then follow the link provided under &lt;strong&gt;Routes&lt;/strong&gt;, as in Figure 4.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/fig4_7.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/fig4_7.png?itok=86i5Kx3e" width="600" height="498" alt="Screenshot showing how to get service and route information for the application." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; ​​​​​​​​​​​​​​Figure 4: Getting service and route information for the application. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt;That will take you to the default Next.js starter app that you created earlier. The welcome screen should look like Figure 5.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/fig5_6.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/fig5_6.png?itok=XeIsMBcG" width="600" height="465" alt="Welcome sreen of the default Next.js starter app." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 5: Default Next.js starter app. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h2&gt;An easier way&lt;/h2&gt; &lt;p&gt;I've talked you through the "hard way" of deploying this application so you'd understand a bit about &lt;code&gt;ImageStream&lt;/code&gt;s and &lt;code&gt;BuildConfig&lt;/code&gt;s. But in fact, it could have been even easier than that.&lt;/p&gt; &lt;p&gt;We could have simply selected &lt;strong&gt;Import from Git&lt;/strong&gt; on the &lt;strong&gt;Add&lt;/strong&gt; page, provided the URL for our GitHub repository, accepted all the defaults except for setting the target port to 3000, and selected &lt;strong&gt;Create&lt;/strong&gt;. This would have created the &lt;code&gt;ImageStream&lt;/code&gt;, &lt;code&gt;BuildConfig&lt;/code&gt;, &lt;code&gt;Deployment&lt;/code&gt;, service and route automatically for us! You can't get much easier than that.&lt;/p&gt; &lt;h2&gt;Wrapping up&lt;/h2&gt; &lt;p&gt;In this article, you've learned how to build a Next.js application using the Red Hat ubi8 Node.js containers, and how to easily build and deploy that application in OpenShift. If you want to use Next.js and are a Red Hat customer, it's good to know how easy it is to use it with the supported Red Hat Node.js container images and then get it deployed to your OpenShift environment.&lt;/p&gt; &lt;p&gt;If you want to learn more about what Red Hat is up to on the Node.js front, check out &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;our Node.js page&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/11/23/how-deploy-nextjs-applications-red-hat-openshift" title="How to deploy Next.js applications to Red Hat OpenShift"&gt;How to deploy Next.js applications to Red Hat OpenShift&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Michael Dawson</dc:creator><dc:date>2022-11-23T07:00:00Z</dc:date></entry><entry><title type="html">Quarkus Tools for IntelliJ 1.14.0 released!</title><link rel="alternate" href="https://quarkus.io/blog/intellij-quarkus-tools-1.14.0/" /><author><name>Jeff Maury</name></author><id>https://quarkus.io/blog/intellij-quarkus-tools-1.14.0/</id><updated>2022-11-23T00:00:00Z</updated><content type="html">We are very pleased to announce the 1.14.0 release of Quarkus Tools for IntelliJ. This release adds support for CodeActions and Quick Fixes. CodeActions / Quick Fixes When an error is detected on one of your Quarkus project files, it is highlighted in the source editor (for instance when you...</content><dc:creator>Jeff Maury</dc:creator></entry><entry><title type="html">WildFly: How to rollback CLI changes</title><link rel="alternate" href="http://www.mastertheboss.com/jbossas/jboss-script/wildfly-how-to-rollback-cli-changes/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/jbossas/jboss-script/wildfly-how-to-rollback-cli-changes/</id><updated>2022-11-22T12:50:43Z</updated><content type="html">In this short article we will learn how to rollback WildFly Command Line commands after their execution in order to restore the original XML configuration. WildFly Configuration History The key concept is that WildFly uses the JBOSS_HOME/standalone/configuration/standalone_xml_history folder to store configuration changes. Within this folder you will find the current configuration history, the configuration history ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title>How I developed a faster Ruby interpreter</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/11/22/how-i-developed-faster-ruby-interpreter" /><author><name>Vladimir Makarov</name></author><id>d5200cd6-1ab5-4212-8e66-fda53f090fe1</id><updated>2022-11-22T07:00:00Z</updated><published>2022-11-22T07:00:00Z</published><summary type="html">&lt;p&gt;In this article, I will describe my efforts to implement a faster interpreter for CRuby, the &lt;a href="https://developers.redhat.com/topics/ruby/all"&gt;Ruby&lt;/a&gt; language interpreter, using a dynamically specialized internal representation (IR). I believe this article will interest developers trying to improve the interpreter performance of dynamic programming languages (e.g., &lt;a href="https://github.com/python/cpython"&gt;CPython&lt;/a&gt; developers).&lt;/p&gt; &lt;p&gt;I will cover the following topics:&lt;/p&gt; &lt;ul&gt;&lt;li&gt; &lt;p&gt;Existing CRuby interpreter and just-in-time (JIT) compilers for Ruby—MJIT, YJIT, and the MIR-based CRuby JIT compiler at the very early stages of development—along with my motivation to start this project and my initial expectations for the project outcome.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;The general approach to performance improvement by specialization and the specializations used in my project to implement a faster CRuby interpreter. I will describe a new dynamically specialized internal representation called SIR, which speeds up the CRuby interpreter in the CRuby virtual machine (VM).&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Implementation and current status.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Performance results in comparison with the base interpreter and other CRuby JIT compilers.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;My future plans for this project and the significance of my work for developers.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt;&lt;h2 id="the-project-motivation"&gt;The project motivation&lt;/h2&gt; &lt;p&gt;About four years ago, I started a MIR project to address shortcomings of the current CRuby JIT compiler, &lt;a href="https://bugs.ruby-lang.org/projects/ruby/wiki/MJIT"&gt;MJIT&lt;/a&gt;. I started MIR as a lightweight, universal JIT compiler, which could be useful for implementing JIT compilers for Ruby and other programming languages.&lt;/p&gt; &lt;p&gt;MIR is already used for the JIT compilers of several programming languages.&lt;/p&gt; &lt;p&gt;Still, I realize that we can't use the current state of MIR to implement good JIT compilers for dynamic programming languages. Therefore, I've been working on new features. You can read about these features in my previous article, &lt;a href="https://developers.redhat.com/articles/2022/02/16/code-specialization-mir-lightweight-jit-compiler"&gt;Code specialization for the MIR lightweight JIT compiler&lt;/a&gt;. In brief, these features include:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;A generalized approach of propagation of dynamic properties of source code based on &lt;em&gt;lazy basic block versioning&lt;/em&gt; and generation of specialized code according to the properties&lt;/li&gt; &lt;li&gt;&lt;em&gt;Trace&lt;/em&gt; generation and optimization based on basic block cloning&lt;/li&gt; &lt;li&gt;A &lt;em&gt;metatracing&lt;/em&gt; MIR C compiler, the project's ultimate goal&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Implementation of these features has taken me many years. The recent success of Shopify's &lt;a href="https://github.com/Shopify/yjit"&gt;YJIT&lt;/a&gt; compiler made me rethink my strategy and find a faster way to implement a MIR-based JIT compiler for CRuby.&lt;/p&gt; &lt;p&gt;To implement a decent MIR-based JIT compiler, I decided earlier to develop some features with a design specific to CRuby. I use dynamically specialized instructions for the CRuby VM and generate machine code from the instructions using the MIR compiler in its current state.&lt;/p&gt; &lt;p&gt;Implementing a dynamically specialized IR and an interpreter for it is beneficial, even without a JIT compiler. The resulting design permits the implementation of a faster CRuby without the complexity of JIT compilers and their portability issues.&lt;/p&gt; &lt;p&gt;YJIT is a very efficient JIT compiler, so I have been assessing its features and asking what other features could make a compiler better.&lt;/p&gt; &lt;p&gt;It is essential to start by understanding how much code is covered by a type of optimization. Some optimizations are limited to a single VM instruction.&lt;/p&gt; &lt;p&gt;Most optimizations work within a &lt;em&gt;basic block&lt;/em&gt;, a group of instructions that run sequentially without internal loops or conditional statements. In Ruby, each basic block is enclosed within braces and is often the body of an innermost loop or an &lt;code&gt;if&lt;/code&gt; statement. Optimizations that span more than a single basic block are much more difficult.&lt;/p&gt; &lt;p&gt;For each stack instruction in the VM, YJIT currently generates the best machine code seen in the field. YJIT generates even faster code than the leading open source C compilers, &lt;a href="https://gcc.gnu.org/"&gt;GCC&lt;/a&gt; and &lt;a href="https://llvm.org/"&gt;LLVM Clang&lt;/a&gt;, for a single VM instruction in the interpreter.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: YJIT was initially written in C but was rewritten in &lt;a href="https://developers.redhat.com/topics/rust"&gt;Rust&lt;/a&gt; to simplify the porting of YJIT to new architectures. The Rust implementation employs abstractions provided currently by the Shopify Ruby team. In my opinion, the best tool to implement a portable YJIT would have been the &lt;a href="https://luajit.org/dynasm.html"&gt;DynASM&lt;/a&gt; C library.&lt;/p&gt; &lt;p&gt;Another compiler technique used by YJIT is &lt;em&gt;basic block versioning&lt;/em&gt;, a powerful technique for dynamically executed languages such as Python and Ruby. I'll describe basic block versioning in the upcoming section, &lt;a href="#specialization-by-lazy-bb-versioning"&gt;Lazy basic block versioning&lt;/a&gt;. The essential idea is that many versions, each with different compiler instructions, exist for each basic block. Some versions are specialized for certain conditions, such as particular data types, and are more efficient than the non-specialized versions when the right conditions hold. The compiler can use the more efficient versions when possible and fall back on less efficient versions under different conditions.&lt;/p&gt; &lt;p&gt;But YJIT's code generation does not span multiple VM instructions. Register transfer language (RTL) is another technique available to compilers, which optimizes code across several stack VM instructions. So I started my new project hoping that, if I implement RTL and use basic block versioning similar to YJIT for some benchmarks, I can match the performance of YJIT even in the interpreter.&lt;/p&gt; &lt;p&gt;I will reveal what I achieved later in this article.&lt;/p&gt; &lt;h3&gt;Code specialization&lt;/h3&gt; &lt;p&gt;I have mentioned "code specialization" several times, but what is specialization? The Merriam-Webster dictionary provides the following definition of the word which is suitable for our purposes: &lt;em&gt;to&lt;/em&gt; &lt;em&gt;design, train, or fit for one particular purpose.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;If we generate code optimized for a particular purpose that happens to be a frequent use case, our code will work faster in most cases. Specialization is one common approach to generating faster code. Even static compilers generate specialized code. For instance, they can generate code specialized for a particular processor model, such as a matrix multiplication that fits a particular size of processor cache.&lt;/p&gt; &lt;p&gt;Specialized code also exists in CRuby. For example, the CRuby VM has specialized instructions for calling methods that operate on numbers, the most frequently used data types. The instructions have specialized names such as &lt;code&gt;opt_plus&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;The compiler can accomplish code specialization statically or dynamically during program execution. Dynamic specialization adds execution time but is hopefully more efficient because interpreters and JIT compilers have more data during the run to help select a particular case for specialization. That is why JIT compilers usually do more specialization than static compilers.&lt;/p&gt; &lt;p&gt;You can specialize &lt;em&gt;speculatively&lt;/em&gt; even when you cannot guarantee that particular conditions for specialization will always be true. For instance, if a Ruby variable is set to an integer once, you can safely speculate that future assignments to that variable will also be integers. Of course, this assumption will occasionally prove untrue in a dynamically typed language such as Ruby.&lt;/p&gt; &lt;p&gt;Therefore, during speculative specialization, you need guards to check if the initial conditions for specialization still hold true. If these conditions are not true, you switch to less efficient code that does not require those conditions for correct execution. Such code switching is commonly called &lt;em&gt;deoptimization&lt;/em&gt;. Guards are described in the upcoming sections, &lt;a href="#specialization-by-lazy-bb-versioning"&gt;Lazy basic block versioning&lt;/a&gt; and &lt;a href="#profile-based-specialization"&gt;Profile-based specialization&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The more dynamic a programming language, the more specialization and the more speculative specialization you need to achieve performance close to static programming languages.&lt;/p&gt; &lt;h2&gt;8 Optimization techniques&lt;/h2&gt; &lt;p&gt;The following subsections describe the eight techniques I have added to my interpreter and MIR-based JIT compiler.&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Dynamically specialized CRuby instructions&lt;/li&gt; &lt;li&gt;RTL code instructions&lt;/li&gt; &lt;li&gt;Hybrid stack/RTL instructions&lt;/li&gt; &lt;li&gt;Type-specialized instructions&lt;/li&gt; &lt;li&gt;Lazy basic block versioning&lt;/li&gt; &lt;li&gt;Profile-based specialization&lt;/li&gt; &lt;li&gt;Specialized iterator instructions&lt;/li&gt; &lt;li&gt;Dynamic flow of specialized instructions&lt;/li&gt; &lt;/ol&gt;&lt;h3 id="dynamically-specialized-cruby-instructions"&gt;1. Dynamically specialized CRuby instructions&lt;/h3&gt; &lt;p&gt;All the specialization I am implementing for CRuby is done &lt;em&gt;dynamically&lt;/em&gt; and &lt;em&gt;lazily&lt;/em&gt;. Currently, I optimize only on the level of a basic block.&lt;/p&gt; &lt;p&gt;I use specialized hybrid stack/RTL instructions. This kind of specialization could be done at compile time, but my interpreter does it lazily as part of a larger technique of generating several different specialized basic blocks. Laziness helps to save memory and time spent on RTL generation. I will explain later why I use hybrid stack/RTL instructions instead of pure RTL.&lt;/p&gt; &lt;p&gt;I also generated &lt;em&gt;type-specialized&lt;/em&gt; instructions. This can be done by &lt;a href="https://arxiv.org/abs/1411.0352"&gt;lazy basic block versioning&lt;/a&gt;, invented by Maxime Chevalier-Boisvert, and used as a major optimization mechanism in YJIT. This optimization is cost-free, and no special guards are needed for checking the value types of instruction input operands. Type specialization is also based on profiling program execution. In this case, the interpreter needs guards to check the types of instruction input operands. Such type of specialization helps to improve cost-free type specialization even further.&lt;/p&gt; &lt;p&gt;Other specializations are based on profile information. Additionally, I included specialized instructions for method calls and accesses to array elements, instance variables, and attributes. The most interesting case is iterator specialization, which I will describe later.&lt;/p&gt; &lt;h3 id="rtl"&gt;2. RTL code instructions&lt;/h3&gt; &lt;p&gt;CRuby uses stack instructions in its VM. Such VM instructions address values implicitly. We can also use VM instructions addressing values explicitly. A set of such instructions is called a register transfer language (RTL).&lt;/p&gt; &lt;p&gt;Here is an example of how the addition of two values is represented by stack instructions and by the RTL instructions generated by my compiler. The number sign (#) is used to start a comment in both languages:&lt;/p&gt; &lt;table border="1" cellpadding="1" cellspacing="1" width="541"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;Stack instructions&lt;/td&gt; &lt;td&gt;RTL instructions&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;getlocal v1 # push v1&lt;/p&gt; &lt;p&gt;getlocal v2 # push v2&lt;/p&gt; &lt;p&gt;opt_plus # pop v1 and v2 push v1 + v2&lt;/p&gt; &lt;p&gt;setlocal res # pop stack value and assign it to res&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt;sir_plusvvv res, v1, v2 # assign v1 + v2 to res&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;As a rule, RTL code contains fewer instructions than stack-based instructions, and as result spends less time in interpreter instruction dispatch code. But RTL sometimes spends more time in operand decoding. More importantly, RTL code results in less memory traffic, because local variables and stack values are addressed directly by RTL instructions. Therefore, stack pushes and pops of local variable values are not as necessary as they are when using stack instructions.&lt;/p&gt; &lt;p&gt;In many cases, CRuby works with values in a stack manner. For example, when pushing values for method calls. So pure RTL has its own disadvantages in such cases and might result in larger code that decodes operands more slowly. Another Ruby-specific problem in RTL lies in implementing fast addressing of Ruby local variables and stack values. Figure 1 shows a typical frame from a Ruby method.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/stack2.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/stack2.png?itok=gqIjWorB" width="380" height="226" alt="The ep pointer separates the local variables from the stack variables in a Ruby method frame" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: The ep pointer separates the local variables from the stack variables in a Ruby method frame. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt;Addressing values in this kind of stack frame is simple. You just use an index relative to &lt;code&gt;ep&lt;/code&gt; (environment pointer): Negative indices for local variables and positive indices for stack values.&lt;/p&gt; &lt;p&gt;Unfortunately, a method's frame could also look like Figure 2.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/stack3.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/stack3.png?itok=RD1WYOnP" width="520" height="226" alt="Another type of frame has the same layout, but inserts an unpredictable distance between the ep pointer and the stack variables" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: Another type of frame has the same layout, but inserts an unpredictable distance between the ep pointer and the stack variables. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt;For this kind of frame, you need to use &lt;code&gt;ep&lt;/code&gt; for local variables and &lt;code&gt;sp&lt;/code&gt; (stack pointer) for the stack. Other ways of addressing could be used, but they all depend on addressing local and stack variables differently. This means a lot of branches for addressing instruction values.&lt;/p&gt; &lt;p&gt;Still, I used RTL about four years ago, and at that time it gave about a 30% improvement on average on a set of microbenchmarks.&lt;/p&gt; &lt;h3 id="hybrid-stack-rtl-instructions"&gt;3. Hybrid stack/RTL instructions&lt;/h3&gt; &lt;p&gt;Based on my previous experience, I modified my old approach of using RTL and started to use hybrid stack/RTL instructions. These instructions can address some operands implicitly and others explicitly.&lt;/p&gt; &lt;p&gt;RTL instructions are generated only on the level of a basic block, and only lazily on the first execution of a basic block. Figure 3 shows the RTL instructions I added.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/JIT-fig3.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/JIT-fig3.png?itok=UoK68-Up" width="544" height="662" alt="RTL instructions naming format." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3: The names of RTL instructions follow a format. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt; &lt;/p&gt; &lt;p&gt;Here, the suffix (final letter) holds the following meanings:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;s&lt;/code&gt;: The value is on the stack.&lt;/li&gt; &lt;li&gt;&lt;code&gt;v&lt;/code&gt;: The value is in a local variable.&lt;/li&gt; &lt;li&gt;&lt;code&gt;i&lt;/code&gt;: The value is an immediate operand.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Some combinations of suffixes are not used. For example, the suffix &lt;code&gt;sss&lt;/code&gt; is not used because an instruction with such a suffix would actually be an existing CRuby stack instruction.&lt;/p&gt; &lt;p&gt;It seems that adding many new VM instructions might result in worse code locality in the interpreter. But in practice, the benefits of reduced dispatching and stack memory traffic outweigh code locality problems. In general code, locality is less important than data locality in modern processors. Just the introduction of hybrid stack/RTL instructions can improve the performance of some benchmarks by 50%.&lt;/p&gt; &lt;h3 id="type-specialized-instructions"&gt;4. Type-specialized instructions&lt;/h3&gt; &lt;p&gt;Integers in the CRuby VM are represented by &lt;a href="https://gmplib.org/"&gt;multi-precision integers&lt;/a&gt; or by &lt;code&gt;fixnum&lt;/code&gt;, a &lt;a href="https://stackoverflow.com/questions/33843393/how-does-ruby-differentiate-value-with-value-and-pointer"&gt;tagged&lt;/a&gt; integer value that fits in one machine word. Floating-point numbers are represented where possible by tagged &lt;a href="https://standards.ieee.org/ieee/754/6210/"&gt;IEEE-754&lt;/a&gt; double values called &lt;code&gt;flonum&lt;/code&gt;, and otherwise by IEEE-754 values in the CRuby heap.&lt;/p&gt; &lt;p&gt;Many CRuby VM instructions are designed to work on primitive data types like a &lt;code&gt;fixnum&lt;/code&gt;. The instructions make a lot of checks before executing the actual operation. For example, &lt;code&gt;opt_plus&lt;/code&gt; checks that input data is &lt;code&gt;fixnum&lt;/code&gt; and that the Ruby &lt;code&gt;+&lt;/code&gt; operator is not redefined for integers. If the checks fail, a general &lt;code&gt;+&lt;/code&gt; method is called.&lt;/p&gt; &lt;p&gt;Type-specialized instructions allowed me to remove the checks and the call code. The optimization resulted in faster and slimmer VM instructions and better interpreter code locality. The new type-specialized instructions are shown in Figure 4.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/JIT-fig4.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/JIT-fig4.png?itok=NqE4UKjv" width="1440" height="414" alt="The type-specialized instructions naming format." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4: The names of type-specialized instructions follow this format. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt; &lt;/p&gt; &lt;p&gt;The prefix takes on the following meanings:&lt;/p&gt; &lt;p&gt;&lt;code&gt;sir_i&lt;/code&gt;: Denotes instructions specialized for &lt;code&gt;fixnum&lt;/code&gt; (integers).&lt;/p&gt; &lt;p&gt;&lt;code&gt;sir_f&lt;/code&gt;: Denotes instructions specialized for &lt;code&gt;flonum&lt;/code&gt; (floating-point numbers).&lt;/p&gt; &lt;p&gt;&lt;code&gt;sir_ib&lt;/code&gt;: Used for branch and &lt;code&gt;fixnum&lt;/code&gt; compare instructions.&lt;/p&gt; &lt;p&gt;To guarantee that type-specialized instructions are passed data of the expected types, lazy basic block versioning can be used.&lt;/p&gt; &lt;p&gt;The type-specialized instructions can be also generated from the profile information. In this case, type guards guarantee data of the expected types.&lt;/p&gt; &lt;p&gt;If an exceptional event prevents the remainder of a basic block from executing, the interpreter deoptimizes code by switching to RTL code for the basic block, which is not type-specialized. An example of an exceptional event could be a &lt;code&gt;fixnum&lt;/code&gt; overflow, which requires a multi-precision number result instead of the expected &lt;code&gt;fixnum&lt;/code&gt;. Hybrid stack/RTL and type-specialized instructions are designed not to do any instruction data moves before the switch, which would require pushing variable values on the stack.&lt;/p&gt; &lt;p&gt;The same deoptimization happens if a standard Ruby operator, such as integer &lt;code&gt;+&lt;/code&gt;, is redefined. The deoptimization removes all basic block clones containing type-specialized instructions for this operation, because there is a tiny probability that these basic block clones will be used in the future.&lt;/p&gt; &lt;p&gt;Instructions specialized for &lt;code&gt;flonum&lt;/code&gt; values hardly improve interpreter performance, because most of the instruction execution time is spent tagging and untagging &lt;code&gt;flonum&lt;/code&gt; values, requiring many shifts and logical operations. Therefore, I included the instructions specialized for &lt;code&gt;flonum&lt;/code&gt; values mostly for a future MIR-based JIT compiler, which will remove redundant floating-point number tagging and untagging.&lt;/p&gt; &lt;h3 id="specialization-by-lazy-bb-versioning"&gt;5. Lazy basic block versioning&lt;/h3&gt; &lt;p&gt;This technique is easiest to explain by an example. Take the following &lt;code&gt;while&lt;/code&gt; loop:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-ruby"&gt;while i &lt; 100 do i += 1 end&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Figure 5 illustrates basic block versioning for this loop.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/bbv.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/bbv.png?itok=ty8wm4Oj" width="740" height="860" alt="Basic block versioning creates several alternative ways to step through a basic block." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 5: Basic block versioning creates several alternative ways to step through a basic block. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt; &lt;/p&gt; &lt;p&gt;Upon its first encounter with the basic block, the interpreter doesn't know the types of values on the stack or in variables. But when we execute &lt;code&gt;BB1&lt;/code&gt; for the first time (see the first diagram), we can easily figure out that the value type of &lt;code&gt;i&lt;/code&gt; became &lt;code&gt;fixnum&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;In the top left diagram, the successor of &lt;code&gt;BB1&lt;/code&gt; is &lt;code&gt;BB3&lt;/code&gt; and there is only one version of &lt;code&gt;BB3&lt;/code&gt;, which has no knowledge of variable value types. So we clone &lt;code&gt;BB3&lt;/code&gt; to create &lt;code&gt;BB3v2&lt;/code&gt;, in which the value type of &lt;code&gt;i&lt;/code&gt; is always &lt;code&gt;fixnum&lt;/code&gt;. We make &lt;code&gt;BB3v2&lt;/code&gt; a successor of &lt;code&gt;BB1&lt;/code&gt; (see the second diagram) and start executing &lt;code&gt;BB3v2&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;From &lt;code&gt;BB3v2&lt;/code&gt;, since we haven't completed the loop, we go to &lt;code&gt;BB4&lt;/code&gt;. No variable value is changed in &lt;code&gt;BB3v2&lt;/code&gt;. So at the end of &lt;code&gt;BB3v2&lt;/code&gt; the value type of &lt;code&gt;i&lt;/code&gt; is still &lt;code&gt;fixnum&lt;/code&gt;. Therefore, we can create &lt;code&gt;BB4v2&lt;/code&gt; and make it a successor of &lt;code&gt;BB3v2&lt;/code&gt;. Because we know that the value type of &lt;code&gt;i&lt;/code&gt; is &lt;code&gt;fixnum&lt;/code&gt; at the beginning of &lt;code&gt;BB4v2&lt;/code&gt;, we can easily deduce that the type of &lt;code&gt;i&lt;/code&gt; at the end of &lt;code&gt;BB4v2&lt;/code&gt; is also &lt;code&gt;fixnum&lt;/code&gt;. We need a version of &lt;code&gt;BB4v2&lt;/code&gt;'s successor in which the type of &lt;code&gt;i&lt;/code&gt; is &lt;code&gt;fixnum&lt;/code&gt;. Fortunately, such a version already exists: &lt;code&gt;BB3v2&lt;/code&gt;. So we just change the successor of &lt;code&gt;BB4v2&lt;/code&gt; to &lt;code&gt;BB3v2&lt;/code&gt; (see the third diagram) and execute &lt;code&gt;BB3v2&lt;/code&gt; again.&lt;/p&gt; &lt;p&gt;In short, knowing the type of &lt;code&gt;i&lt;/code&gt; permits the interpreter to specialize instructions in &lt;code&gt;BB3v2&lt;/code&gt; and &lt;code&gt;BB4v2&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;As you can see, we create basic block versions only when a preceding block is actually executed at run time. This is why such basic block versioning is called &lt;em&gt;lazy&lt;/em&gt;. Usually, we create only a few versions of each basic block. But in pathological cases (which can be demonstrated) a huge number of versions can be created. Therefore, we place a limit on the maximum number of versions for one basic block. When we reach this number, we use an already existing basic block version (usually a version with unknown value types) instead of creating a new one.&lt;/p&gt; &lt;p&gt;Basic block versioning can be also used for other specialization techniques besides type-specialization.&lt;/p&gt; &lt;h3 id="profile-based-specialization"&gt;6. Profile-based specialization&lt;/h3&gt; &lt;p&gt;When the interpreter can't find out the input data types from basic block versioning (e.g., when handling the result of a polymorphic type method call), we insert a &lt;code&gt;sir_inspect_stack_type&lt;/code&gt; or &lt;code&gt;sir_inspect_type&lt;/code&gt; profiling instruction to inspect the types of the stack values or local variables. After the number of executions of a basic block version reaches some threshold, we generate a basic block version with speculatively type-specialized instructions for the data types we found, instead of profiling instructions. Figure 6 shows the format of names of speculatively type-specialized instructions.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/speculative-insns-1.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/speculative-insns-1.png?itok=zL0iWxC7" width="600" height="261" alt="Speculative instructions" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 6: The names of speculative instructions follow this format. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt;The speculative instructions check the value types of the operands. If the operand doesn't have the expected type, the instruction switches to a non-type specialized version of the basic block.&lt;/p&gt; &lt;p&gt;Figure 7 shows some possible changes made from &lt;code&gt;sir_inspect_type&lt;/code&gt;, used for profiling the types of local variable values.  Instructions &lt;code&gt;sir_inspect_type&lt;/code&gt;, &lt;code&gt;sir_inspect_fixtype&lt;/code&gt;, and &lt;code&gt;sir_inspect_flotype&lt;/code&gt; are self-modified.  Depending on the types of the inspected values at the profiling stage, instead of &lt;code&gt;sir_inspect_type&lt;/code&gt; we will have &lt;code&gt;sir_inspect_fixtype&lt;/code&gt; (if we observed only fixnum types), &lt;code&gt;sir_inspect_flotype&lt;/code&gt; (if we observed only flonum types) or &lt;code&gt;nop&lt;/code&gt; in all other cases.  After the profiling stage we removes all inspect instructions and nops and can generate speculative instructions from non-type-specialized RTL instructions affected by the inspect instructions, e.g. we can generate speculative instruction &lt;code&gt;sir_simultsvv&lt;/code&gt; instead of non-type-specialized RTL instruction &lt;code&gt;sir_multsvv&lt;/code&gt; if we observed that the instruction input values were only of fixnum type.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/inspect-new_0.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/inspect-new_0.png?itok=v6xmC7rJ" width="600" height="302" alt="Possible changes made from sir_inspect_type, used for profiling types of local variable values" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 7: Possible changes made by sir_inspect_type, used for profiling types of local variable values. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt;The speculative instructions check data types lazily, only when we do something with the data except data moves. Speculatively type-specialized instructions permit the interpreter to use more of the new non-speculative type-specialized instructions in a basic block version. In these cases, the speculative instructions act as type guards for values used in the subsequent instructions.&lt;/p&gt; &lt;p&gt;Additionally, based on the profile information, the original VM call instructions can be specialized to instructions for C function calls, calls to an iseq (a sequence of VM instructions), or accesses to instance variables.&lt;/p&gt; &lt;h3 id="iterators"&gt;7. Specialized iterator instructions&lt;/h3&gt; &lt;p&gt;Many standard Ruby methods are implemented in C. Some of these methods accept a Ruby block represented by an iseq and behave as iterators.&lt;/p&gt; &lt;p&gt;To execute the Ruby block for each iteration, the C code calls the interpreter. This is a very expensive procedure involving a call to &lt;code&gt;setjmp&lt;/code&gt; (also used to implement CRuby exception handling). We can avoid invoking the interpreter by replacing the method call with specialized iterator instructions:&lt;/p&gt; &lt;pre&gt; &lt;code&gt; sir_iter_start start_func sir_cfunc_send =&gt; Lcont: sir_iter_body Lexit, block_bbv, cond_func sir_iter_cont Lcont, arg_func Lexit:&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The iterator instructions are:&lt;/p&gt; &lt;ul&gt;&lt;li&gt; &lt;p&gt;&lt;code&gt;sir_iter_start start_func&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;start_func&lt;/code&gt; checks the receiver type and sets up block arguments.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;code&gt;sir_iter_body exit_label, block_bbv, cond_func&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;cond_func&lt;/code&gt; finishes the iteration or calls &lt;code&gt;block_bbv&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;code&gt;sir_iter_cont cont_label, arg_func&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;arg_func&lt;/code&gt; updates block arguments and permits a goto to &lt;code&gt;cont_label&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Iterator instructions keep their temporary data on the stack. An example of such data is the current array index for an array &lt;code&gt;each&lt;/code&gt; method.&lt;/p&gt; &lt;p&gt;Currently, iterator instructions are implemented only for the &lt;code&gt;fixnum&lt;/code&gt; &lt;code&gt;times&lt;/code&gt; method and for the range and array &lt;code&gt;each&lt;/code&gt; methods. Adding other iterators is easy and straightforward. Usually, you just need to write three very small C functions.&lt;/p&gt; &lt;p&gt;Such specialized iterator instructions can significantly improve performance of Ruby's built-in iterator methods.&lt;/p&gt; &lt;h3 id="dynamic-flow-of-specialized-instructions"&gt;8. Dynamic flow of specialized instructions&lt;/h3&gt; &lt;p&gt;CRuby's front-end, just like the one in CRuby's original implementation, compiles source code into iseq sequences. For each iseq we also create a &lt;em&gt;stub&lt;/em&gt;, an instruction that is executed once and provides the starting point for executing the iseq.&lt;/p&gt; &lt;p&gt;Most executions of basic blocks can rely on assumptions we make about data types during compilation and execution. If we find, during execution, that our speculative assumptions do not hold, we switch back to non-type-specialized hybrid stack/RTL instructions.&lt;/p&gt; &lt;p&gt;For the speculatively type-specialized instructions, the switch can happen when the input value types are not of the expected types. An example situation that violates a speculative assumption for type-specialized instructions is an integer overflow that switches the result type from a &lt;code&gt;fixnum&lt;/code&gt; to a multi-precision number.&lt;/p&gt; &lt;p&gt;Figure 8 shows the flow through a basic block. The downward arrows show the flow that takes place so long as assumptions about data types are valid. When an assumption is invalidated, the path shown by the upward arrows is taken.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/sirflow.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/sirflow.png?itok=4SR2HhBD" width="600" height="495" alt="The interpreter creates type-specialized instructions and reverts to non-specialized instructions when necessary" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 8: The interpreter creates type-specialized instructions and reverts to non-specialized instructions when necessary. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt;Execution of a stub creates hybrid stack/RTL instructions for the first basic block of the iseq. At this point, we also generate type-specialized instructions from type information that we know, and profile instructions for values of unknown type. These type-specialized instructions will be the new execution starting point of the iseq during further iterations.&lt;/p&gt; &lt;p&gt;After that, execution continues from the new type-specialized instructions in the basic block. Possible continuations of the basic block at this point are also stubs for successor basic blocks. When the stub of a successor basic block runs, we create hybrid stack/RTL instructions and instructions specialized by type information collected from types in the predecessor basic block. Profiling instructions are also added here. Execution then proceeds with the new basic blocks.&lt;/p&gt; &lt;p&gt;After a specified number of executions of the same basic block, we generate type-specialized instructions and instructions specialized from the collected profile information. And this is now the new starting point of the basic block. The type-specialized and profile-specialized basic block is also a starting point for the MIR-based JIT compiler I am working on. The current MIR-based compiler generates code from the type-specialized and profile-specialized instructions of one basic block. In the future, the compiler will also generate code from the type-specialized and profile-specialized instructions of the entire method.&lt;/p&gt; &lt;h2 id="implementation-and-the-current-status"&gt;Current status of the implementation&lt;/h2&gt; &lt;p&gt;My interpreter and MIR-based JIT compiler that use the specialized IR can be found in &lt;a href="https://github.com/vnmakarov/ruby"&gt;my GitHub repository&lt;/a&gt;. The current state is good only for running the benchmarks I discuss later. Currently, specialized IR generation and execution is implemented in about 3,500 lines of C code. The generator of C code for MIR is about 2,500 lines. The MIR-based JIT compiler needs to build and install the &lt;a href="https://github.com/vnmakarov/mir/tree/bbv"&gt;MIR library&lt;/a&gt;, whose size is about 900KB of machine code. Although his library can be shrunk.&lt;/p&gt; &lt;p&gt;To use the interpreter with the specialized IR, run a program with the &lt;code&gt;--sir&lt;/code&gt; option. There is also an &lt;code&gt;--sir-max-versions=&lt;em&gt;n&lt;/em&gt;&lt;/code&gt; option for setting the maximum number of versions of a basic block.&lt;/p&gt; &lt;p&gt;To use the interpreter with the specialized IR and MIR JIT compiler, run a program with the &lt;code&gt;--mirjit&lt;/code&gt; option.&lt;/p&gt; &lt;p&gt;You can also enable the &lt;code&gt;--sir-debug&lt;/code&gt; and &lt;code&gt;--mirjit-debug&lt;/code&gt; debugging options, but please be aware that the debugger output, even for a small Ruby program, will be pretty big.&lt;/p&gt; &lt;h2 id="benchmarking"&gt;Benchmarking the faster interpreter&lt;/h2&gt; &lt;p&gt;I have benchmarked the faster interpreter against the base CRuby interpreter, YJIT, MJIT, and the MIR-based JIT compiler using the following options:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Interpreter with SIR: &lt;code&gt;--sir&lt;/code&gt;&lt;/li&gt; &lt;li&gt;YJIT: &lt;code&gt;--yjit-call-threshold=1&lt;/code&gt;&lt;/li&gt; &lt;li&gt;MJIT: &lt;code&gt;--jit-min-calls=1&lt;/code&gt;&lt;/li&gt; &lt;li&gt;SIR+MIR: &lt;code&gt;--mirjit&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The run time of each benchmark varies from half a minute to a couple of minutes. The run-time options for each JIT compiler are chosen to generate machine code as soon as possible and thus get the best result for that compiler.&lt;/p&gt; &lt;p&gt;I did the benchmarking on an Intel Core i7-9700K with 16GB memory under Linux Fedora Core 32, using my own microbenchmarks which can be found in the &lt;a href="https://github.com/vnmakarov/ruby/tree/sir-mirjit/sir-bench"&gt;sir-bench directory of my repository&lt;/a&gt; and &lt;a href="https://github.com/mame/optcarrot"&gt;Optcarrot&lt;/a&gt;. Each benchmark was run three times and the best result was chosen.&lt;/p&gt; &lt;p&gt;Note that the MIR-based JIT compiler is in the very early stage of development, and I am expecting significant performance improvements in the future.&lt;/p&gt; &lt;h3 id="micro-benchmarks"&gt;Results from microbenchmarks&lt;/h3&gt; &lt;p&gt;Figure 9 shows the wall times for various microbenchmarks.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/JIT-fig9.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/JIT-fig9.png?itok=28cJTzcA" width="1172" height="668" alt="A bar graph shows the wall times for various microbenchmarks." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 9: Absolute speeds of four JIT compilers are better or worse on different benchmarks. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt; &lt;/p&gt; &lt;p&gt;The following points explain some of the results:&lt;/p&gt; &lt;ol&gt;&lt;li&gt; &lt;p&gt;On Geomean, my new interpreter achieved 109% of the performance of the base CRuby interpreter, but was 6% slower than YJIT.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;RTL with type specialization makes the &lt;code&gt;while&lt;/code&gt; benchmark run faster than YJIT. Using RTL decreases the number of executed instructions per iteration from 8 (stack instructions) to 2 (RTL instructions) and removes 5 CRuby stack accesses.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Iterator specialization permits my interpreter to execute the &lt;code&gt;ntimes&lt;/code&gt;&lt;em&gt; &lt;/em&gt;(nested times) benchmark without leaving and entering the major &lt;code&gt;vm_exec_core&lt;/code&gt; interpreter function. Avoiding that switch results in better code performance. As I wrote earlier, entering the function is very expensive, as it requires a call to the &lt;code&gt;setjmp&lt;/code&gt; C function.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Method calls are where the interpreter's specialization does not work well. YJIT-generated code for the &lt;code&gt;call&lt;/code&gt; benchmark works twice as fast as my interpreter with the specialized IR. YJIT generates code that is already specialized for the call's particular characteristics. For instance, YJIT can reflect the number of arguments and the local variables of the called method. I could add call instructions specialized for these parameters too, but doing so would hugely increase the number of specialized instructions. So I decided not to even try this approach, especially as such specialization will be solved by the MIR-based JIT compiler.&lt;/p&gt; &lt;/li&gt; &lt;/ol&gt;&lt;p&gt;People often measure only wall time for benchmarks. But CPU use is important too. It reflects how much energy is spent executing the code. CPU time improvements are given in Figure 10.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/sir-cpu-new.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/sir-cpu-new.png?itok=j78af0ta" width="576" height="480" alt="CPU time is similar to wall time except for the MJIT compiler." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 10: CPU time is similar to wall time except for the MJIT compiler. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt;Differences in CPU usage are comparable to differences in wall time for the microbenchmarks, except for MJIT. MJIT generates machine code using GCC or LLVM in parallel with Ruby program execution. GCC and LLVM do a lot of optimizations and spend a lot of time in them.&lt;/p&gt; &lt;p&gt;YJIT-based and MIR-based JIT compilers do not generate code in parallel. When they decide to JIT-compile some VM instructions, code execution stops until the machine code for these instructions is ready.&lt;/p&gt; &lt;p&gt;Figure 11 shows the maximum resident memory use of my fast interpreter and different JIT compilers, relative to the basic interpreter.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/sir-mem-new_1.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/sir-mem-new_1.png?itok=tkalQRld" width="576" height="480" alt="YJIT's maximum memory usage is high" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 11: YJIT's maximum memory use is high. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt;YJIT reserves a big pool of memory for its work. This memory is often not fully used. I assume this YJIT behavior can be improved.&lt;/p&gt; &lt;h3 id="optcarrot"&gt;Optcarrot benchmark results&lt;/h3&gt; &lt;p&gt;Optcarrot, a Nintendo game computer emulator, is a classic benchmark for Ruby. Figure 12 shows the best frame per second (FPS) values when 3,000 frames are generated by Optcarrot.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/carrot-new.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/carrot-new.png?itok=tjIRQaUN" width="576" height="480" alt="The new interpreter performs better than the basic interpreter on Optcarrot." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 12: The new interpreter performs better than the basic interpreter on Optcarrot. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt; &lt;/p&gt; &lt;p&gt;The specialized IR shows a 45% improvement over the basic interpreter.&lt;/p&gt; &lt;p&gt;In the optimized version of Optcarrot (Figure 13), a huge method is generated before execution. Basically, the method is a substitute for aggressive method inlining. Because there are a lot fewer method calls, for which specialization in the interpreter is not as good as for JIT compilers, the interpreter with the specialized IR generates the second-best result, right after the MIR-based JIT compiler.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/optcarrot-new.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/optcarrot-new.png?itok=nRXefVdz" width="576" height="480" alt="The new interpreter performs much better than the basic interpreter on optimized Optcarrot." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 13: The new interpreter performs much better than the basic interpreter on optimized Optcarrot. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt; &lt;/p&gt; &lt;p&gt;YJIT behavior is the worst on this benchmark. I did not investigate why YJIT has such low performance in this case. I have heard that the reason might be that YJIT did not implement the &lt;code&gt;opt_case_dispatch&lt;/code&gt; instruction used by the optimized Optcarrot.&lt;/p&gt; &lt;p&gt;Although MJIT produces a decent FPS result, it takes forever to finish the benchmark. Running with MJIT, Ruby terminates only when compilation finishes for all methods currently being compiled by GCC or LLVM. This trait is a specific limitation of the current parallel MJIT engine. Because the huge method requires a lot of time for GCC to compile, the method in this benchmark is actually being executed by the interpreter. The benchmark finishes in a reasonable amount of time, but MJIT is still waiting for the method compilation to finish, even though the generated machine code is never used.&lt;/p&gt; &lt;h2 id="conclusion"&gt;The significance of this prototype for Ruby and Python&lt;/h2&gt; &lt;p&gt;The faster CRuby interpreter described in this article is only a very early prototype. Much needs to be added, and there are still many bugs. I am going to finish the work this year and continue my work on a MIR-based CRuby JIT compiler using the specialized IR. There are still many bugs to fix in the JIT compiler and a lot of work must be done to generate better code.&lt;/p&gt; &lt;p&gt;The specialization described here is useful for developers who already use classical approaches to speed up dynamic programming language interpreters and now want to achieve even better interpreter performance. Currently, I consider the specialized IR and the MIR-based CRuby JIT compiler more as research projects for me than as candidates for production use. The enhancements in the projects demonstrate what can be accomplished with the MIR project.&lt;/p&gt; &lt;h2&gt;What's next?&lt;/h2&gt; &lt;p&gt;Because there is too much technical debt in my different code bases, I probably cannot provide maintenance and adaptation of the code for future CRuby releases. Anyone can use and modify my code for any purpose. I welcome such work. I will provide help where I can, but unfortunately, I cannot commit to this work. However, I am fully committed to maintaining and improving the MIR project.&lt;/p&gt; &lt;p&gt;Currently, CPython developers are working on speeding up their interpreter. Some techniques described here (particularly RTL and basic block versioning) are not used in their project. But these techniques might prove even more profitable than CRuby because CPython uses reference counts for garbage collection. It may be easier to implement the techniques I developed in CPython. Although CPython supports concurrency, it does not have real parallelism as CRuby does.&lt;/p&gt; &lt;p&gt;You can comment below if you have questions. Your feedback is welcome. Developers can also get some practice with hands-on labs in &lt;a href="https://developers.redhat.com/developer-sandbox/activities/learn-kubernetes-using-red-hat-developer-sandbox-openshift"&gt;Developer Sandbox for Red Hat OpenShift&lt;/a&gt; for free.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/11/22/how-i-developed-faster-ruby-interpreter" title="How I developed a faster Ruby interpreter"&gt;How I developed a faster Ruby interpreter&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Vladimir Makarov</dc:creator><dc:date>2022-11-22T07:00:00Z</dc:date></entry><entry><title type="html">Redis Job Queue - Reloaded</title><link rel="alternate" href="https://quarkus.io/blog/redis-job-queue-reloaded/" /><author><name>Clement Escoffier</name></author><id>https://quarkus.io/blog/redis-job-queue-reloaded/</id><updated>2022-11-22T00:00:00Z</updated><content type="html">In How to implement a job queue with Redis, we explained how to implement a job queue mechanism with Redis and the new Redis API from Quarkus. The approach explored in that blog post had a significant flaw: if the execution of a job failed, the request was lost and...</content><dc:creator>Clement Escoffier</dc:creator></entry><entry><title>Why GPUs are essential for AI and high-performance computing</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/11/21/why-gpus-are-essential-computing" /><author><name>Audrey Reznik, Troy Nelson, Kaitlyn Abdo, Christina Xu</name></author><id>452b1181-0613-4a56-8cf6-16ca8e55a3f5</id><updated>2022-11-21T07:00:00Z</updated><published>2022-11-21T07:00:00Z</published><summary type="html">&lt;p&gt;Graphics processing units (GPU) have become the foundation of artificial intelligence. Machine learning was slow, inaccurate, and inadequate for many of today's applications. The inclusion and utilization of GPUs made a remarkable difference to large neural networks. Deep learning discovered solutions for image and video processing, putting things like autonomous driving or facial recognition into mainstream technology.&lt;/p&gt; &lt;p&gt;The connection between GPUs and OpenShift does not stop at data science. High-performance computing is one of the hottest trends in enterprise tech. Cloud computing creates a seamless process enabling various tasks designated for supercomputers, better than any other computing power you use, saving you time and money.&lt;/p&gt; &lt;h2&gt;How GPUs work&lt;/h2&gt; &lt;p&gt;Let’s back up and make sure we understand how GPUs do what they do.&lt;/p&gt; &lt;p&gt;The term, graphics processing unit, was popularized in 1999 when Nvidia marketed its GeForce 256 with the capabilities of graphics transformation, lighting, and triangle clipping. These are math-heavy computations, which ultimately help render three-dimensional spaces. The engineering is tailored towards these actions, which allows processes to be increasingly optimized and accelerated. Performing millions of computations or using floating point values creates repetition. This is the perfect scenario for tasks to be run in parallel.&lt;/p&gt; &lt;p&gt;GPUs can dominate dozens of CPUs in performance with the help of caching and additional cores. Imagine we are attempting to process high-resolution images. For example, if one CPU takes one minute to process a single image, we would be stuck if we needed to go through nearly a million images for a video. It would take several years to run on a single CPU.&lt;/p&gt; &lt;p&gt;Scaling CPUs will linearly speed up the process. However, even at 100 CPUs, the process would take over a week, not to mention adding quite an expensive bill. A few GPUs, with parallel processing, can solve the problem within a day. We made impossible tasks possible with this hardware.&lt;/p&gt; &lt;h2&gt;The evolution of GPUs&lt;/h2&gt; &lt;p&gt;Eventually, the capabilities of GPUs expanded to include numerous processes, such as artificial intelligence, which often requires running computations on gigabytes of data. Users can easily integrate high-speed computing with simple queries to APIs and coding libraries with the help of complementary software packages for these beasts.&lt;/p&gt; &lt;p&gt;In November 2006, NVIDIA introduced CUDA, a parallel computing platform and programming model. This enables developers to use GPUs efficiently by leveraging the parallel compute engine in NVIDIA’s GPUs and guiding them to partition their complex problems into smaller, more manageable problems where each sub-problem is independent of the other's result.&lt;/p&gt; &lt;p&gt;NVIDIA further spread its roots by partnering with Red Hat OpenShift to adapt CUDA to Kubernetes, allowing customers to develop and deploy applications more efficiently. Prior to this partnership, customers interested in leveraging Kubernetes on top of GPUs had to manually write containers for CUDA and software to integrate Kubernetes with GPUs. This process was time-consuming and prone to errors. Red Hat OpenShift simplified this process by enabling the GPU operator to automatically containerize CUDA and other required software when a customer deploys OpenShift on top of a GPU server. &lt;/p&gt; &lt;p&gt;Red Hat OpenShift Data Science (RHODS) expanded the mission of leveraging and simplifying GPU usage for data science workflows. Now when customers start their Jupyter notebook server on RHODS, they have the option to customize the number of GPUs required for their workflow and select Pytorch and TensorFlow GPU-enabled notebook images. You may be able to select 1 or more GPUs, depending on the GPU machine pool added to your cluster. Customers have the power to use GPUs in their data mining and model processing tasks. &lt;/p&gt; &lt;h2&gt;GPUs in RHODS&lt;/h2&gt; &lt;p&gt;Interested in hearing more about using GPUs in RHODS?  Then check out our new learning path &lt;em&gt;Configure a Jupyter notebook to use GPUs for AI/ML modeling&lt;/em&gt;, which can be found under the &lt;a href="https://developers.redhat.com/products/red-hat-openshift-data-science/getting-started"&gt;Getting Started section&lt;/a&gt; in our &lt;a href="https://developers.redhat.com/products/red-hat-openshift-data-science/overview"&gt;RHODS public sandbox&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/11/21/why-gpus-are-essential-computing" title="Why GPUs are essential for AI and high-performance computing"&gt;Why GPUs are essential for AI and high-performance computing&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Audrey Reznik, Troy Nelson, Kaitlyn Abdo, Christina Xu</dc:creator><dc:date>2022-11-21T07:00:00Z</dc:date></entry><entry><title>Modernize at scale with the new migration toolkit for applications</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/11/18/modernize-scale-new-migration-toolkit-applications" /><author><name>Yashwanth Maheshwaram</name></author><id>f8db9619-4659-4a29-819c-b34658a17249</id><updated>2022-11-18T07:00:00Z</updated><published>2022-11-18T07:00:00Z</published><summary type="html">&lt;p&gt;The &lt;a href="https://developers.redhat.com/products/mta/overview"&gt;migration toolkit for applications &lt;/a&gt;from Red Hat equips developers with tools to assess, prioritize, and modernize &lt;a href="https://developers.redhat.com/java"&gt;Java&lt;/a&gt; applications across the hybrid cloud on &lt;a href="https://developers.redhat.com/products/openshift/"&gt;Red Hat OpenShift&lt;/a&gt;. Version 6 of the toolkit is now generally available—and it's a major step up from the previous version, offering new capabilities to accelerate large-scale application modernization efforts. Read on to explore the new toolkit and see a demo.&lt;/p&gt; &lt;p&gt;Based on the open source &lt;a href="https://www.konveyor.io/"&gt;Konveyor&lt;/a&gt; project, the migration toolkit for applications provides insights and alignment for project leads and migration teams as they move to Red Hat OpenShift for a single application or a portfolio of applications. These insights are a huge boost for developers and organizations looking to modernize and migrate from their legacy platforms to the cloud.&lt;/p&gt; &lt;h2&gt;What's new in migration toolkit for applications&lt;/h2&gt; &lt;p&gt;Version 6 of the migration toolkit includes the following tools for your app modernization:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;&lt;strong&gt;New application inventory and assessment modules&lt;/strong&gt; that assist organizations in managing, classifying, and tagging their applications while assessing application suitability for deployment in containers, including flagging potential risks for migration strategies.&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;strong&gt;Full integration with source code and binary repositories&lt;/strong&gt; to automate the retrieval of applications for analysis along with proxy integration, including HTTP and HTTPS proxy configuration managed in the user interface.&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;strong&gt;Improved analysis capabilities&lt;/strong&gt; with new analysis modes, including source and dependency modes that parse repositories to gather dependencies and add them to the overall scope of the analysis. There is also a simplified user experience to configure the analysis scope, including open source libraries.&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;strong&gt;Enhanced role-based access control (RBAC)&lt;/strong&gt; powered by &lt;a href="https://access.redhat.com/products/red-hat-single-sign-on"&gt;Red Hat's single sign-on technology&lt;/a&gt;, defining three new differentiated personas with different permissions to suit the needs of each user—administrator, architect, and migrator—including credentials management for multiple credential types.&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;strong&gt;An administrator perspective&lt;/strong&gt; to provide tool-wide configuration management for administrators.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Included with a Red Hat OpenShift subscription, the migration toolkit for applications helps reduce complexity and risks to accelerate the modernization of your non-cloud-enabled applications. &lt;/p&gt; &lt;h2&gt;&lt;span&gt; &lt;/span&gt;Watch a demo&lt;/h2&gt; &lt;p&gt;To see version 6 of the migration toolkit in action, check out the following demo.&lt;/p&gt; &lt;div class="video-embed-field-provider-youtube video-embed-field-responsive-video"&gt; &lt;/div&gt; &lt;h2&gt;Supported modernization paths&lt;/h2&gt; &lt;p&gt;Wondering if Red Hat's migration toolkit for applications will support your modernization paths? Our team has compiled a list of the most commonly used migration paths, along with the most common use cases for the toolkit. See the &lt;a href="https://developers.redhat.com/products/mta/use-cases"&gt;migration toolkit for applications page&lt;/a&gt; for the full list of supported migration paths.&lt;/p&gt; &lt;h2&gt;Get started with the migration toolkit on OpenShift&lt;/h2&gt; &lt;p&gt;You can install the migration toolkit for applications on OpenShift 4.9+ with an Operator. See &lt;a href="https://developers.redhat.com/products/mta/getting-started"&gt;Get started with the migration toolkit for applications&lt;/a&gt; for instructions.&lt;/p&gt; &lt;p&gt;If you would like to try the command-line interface (CLI), there is also a &lt;a href="https://developers.redhat.com/products/mta/download"&gt;download&lt;/a&gt; option available.&lt;/p&gt; &lt;h2&gt; &lt;/h2&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/11/18/modernize-scale-new-migration-toolkit-applications" title="Modernize at scale with the new migration toolkit for applications"&gt;Modernize at scale with the new migration toolkit for applications&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Yashwanth Maheshwaram</dc:creator><dc:date>2022-11-18T07:00:00Z</dc:date></entry></feed>
